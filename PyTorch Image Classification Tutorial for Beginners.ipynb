{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Image Classification Tutorial for Beginners\n",
    "\n",
    "This is the self-taught PyTorch Image Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2, os\n",
    "\n",
    "import albumentations as A \n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np # Data processing\n",
    "import matplotlib.pyplot as plt # Data visualization\n",
    "from tqdm import tqdm # Progress bar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.__version__)\n",
    "print(A.__version__)\n",
    "print(cv2.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1:Prepare and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"/home/jacobian/Desktop/Master_degree_research/data/Layer2/data/train/\"\n",
    "sub_folders = [\"GOOD\", \"BAD\"]\n",
    "labels = [0, 1]\n",
    "\n",
    "data = [] \n",
    "for s, l in zip(sub_folders, labels):\n",
    "    print(s, l)\n",
    "    for r, d, f in os.walk(root_dir + s):\n",
    "        print(r, d, f )\n",
    "        for file in f:\n",
    "            if \".png\" in file:\n",
    "                \n",
    "                data.append((os.path.join(s, file), l, s))\n",
    "                \n",
    "df = pd.DataFrame(data, columns=[\"file_name\", \"label\", \"classes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.countplot(data= df, x=\"classes\", color='white', edgecolor='black', hatch=['.', '']);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is always a good idea to plot a few samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 3, figsize=(10, 6))\n",
    "\n",
    "idx = 0\n",
    "for i in range(2):\n",
    "    for j in range(3):\n",
    "        \n",
    "        label = df.label[idx]\n",
    "        file_path = os.path.join(root_dir, df.file_name[idx])\n",
    "        \n",
    "        # Read an image with OpenCV\n",
    "        image = cv2.imread(file_path)\n",
    "        \n",
    "        # Convert the image to RGB color space\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Resize image\n",
    "        image = cv2.resize(image, (204, 204))\n",
    "        \n",
    "        ax[i, j].imshow(image)\n",
    "        ax[i, j].set_title(f\"Label: {label} ({'GOOD' if label == 0 else 'BAD'})\")\n",
    "        ax[i, j].axis(\"off\")\n",
    "        \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the dataset into training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(df,\n",
    "                                     test_size = 0.1,\n",
    "                                     random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Build a Baseline\n",
    "\n",
    "1. A data pip-line for loading images\n",
    "2. A **model** with **loss function** and **optimizer**\n",
    "3. A training pipe-line, including a cross-validation strategy\n",
    "\n",
    "\n",
    "Deep Learning has a lot of Experimentation. We should make the code as modular as possible and work with a configuration for tuning;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add configurable parameter\n",
    "from types import SimpleNamespace\n",
    "\n",
    "cfg = SimpleNamespace(**{})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a data pipe-line for loading images\n",
    "\n",
    "* **`Dataset`** class: Loads and preprocesses the dataset. You will need to customize this class for your purpose.\n",
    "* **`Dataloader`** class: Loads batches of data samples to the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 1234):\n",
    "    \"\"\"Set seed for reproducibility across multiple functions and libraries.\n",
    "\n",
    "    Args:\n",
    "        seed (int, optional): seed value to be used. Default is 1234.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Seed Python's built-in random module\n",
    "    # This affects Python random functions, like random.shuffle and random.randint\n",
    "    random.seed(seed)\n",
    "\n",
    "    # Seed the Python hash function\n",
    "    # PYTHONHASHSEED environment variable can be set to control the seed for generating hash of the types covered by Python's \"hash\" mechanism\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "    # Seed for NumPy random functions\n",
    "    # NumPy is used for numerical operations in Python, including generating random numbers for array elements\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Seed PyTorch's random number generator\n",
    "    # PyTorch is a machine learning library, and this seed affects PyTorch random functions\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    # If using a single GPU with CUDA, seed the GPU\n",
    "    # CUDA is a parallel computing platform that allows for using GPUs\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    \n",
    "    # If using more than 1 GPU with CUDA, seed all the GPUs\n",
    "    # This allows for reproducibility across multiple GPUs\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    # Force CUDA to use deterministic algorithms\n",
    "    # CUDA operations can be non-deterministic, causing minor differences in computations, this forces those operations to be deterministic\n",
    "    torch.backends.cudnn.deterministic = True \n",
    "    \n",
    "    # Disabling the inbuilt cuDNN auto-tuner \n",
    "    # The auto-tuner selects the best algorithm for cuDNN operations based on the hardware, \n",
    "    # which could lead to non-deterministic results between multiple runs of the same program.\n",
    "    torch.backends.cudnn.benchmark = False \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.root_dir = \"/home/jacobian/Desktop/Master_degree_research/data/Layer2/data/train/\"\n",
    "cfg.image_size = 256\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 cfg,\n",
    "                 df, transform=None,\n",
    "                 mode = \"val\") -> None:\n",
    "        super().__init__()\n",
    "        self.root_dir = cfg.root_dir\n",
    "        self.df =df\n",
    "        self.file_name = df[\"file_name\"].values\n",
    "        self.labels = df[\"label\"].values\n",
    "    \n",
    "        if transform:\n",
    "            self.transform = transform\n",
    "        else:\n",
    "            self.transform = A.Compose([\n",
    "                A.Resize(cfg.image_size, cfg.image_size),\n",
    "                ToTensorV2()\n",
    "            ])\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get file_path and label for index\n",
    "        label = self.labels[idx]\n",
    "        file_path = os.path.join(self.root_dir, self.file_name[idx])\n",
    "        # print(file_path)\n",
    "        \n",
    "        # Read image with OpenCV\n",
    "        image = cv2.imread(file_path)\n",
    "        # Convert the image to RGB color space.\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        # Apply augmentations\n",
    "        augmented = self.transform(image=image)\n",
    "        image = augmented[\"image\"]\n",
    "        \n",
    "        # Normalize because ToTen\n",
    "        \n",
    "        return image, label\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `Dataloader` to load batch of image into neural network\n",
    "1. Provide `Dataloader` with the instance of the `Dataset` you want to navigate\n",
    "2. The size of batches (`cfg.batch_size`) \n",
    "3. Other information such as: shuffle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.batch_size = 32\n",
    "\n",
    "example_dataset = CustomDataset(cfg, df)\n",
    "\n",
    "example_dataloader = DataLoader(example_dataset,\n",
    "                                batch_size=cfg.batch_size,\n",
    "                                shuffle=True,\n",
    "                                num_workers=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch size should be fixed throughout the training, use the biggest batch size as possible by start from 32, 64, 128, etc. until get memory error and use the last batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (image_batch, label_batch) in example_dataloader:\n",
    "    print(image_batch.shape)\n",
    "    print(label_batch.shape)\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Dataloader` return **image batch** and **label batch**\n",
    "- `image_batch` is tensor of the shape (`32, 3, 256, 256`) (`batch_size, channels, image_height, image_width`)\n",
    "- `label_batch` is a tensor of the shape (`32`)\n",
    "\n",
    "![Image](https://miro.medium.com/v2/resize:fit:1100/format:webp/1*AqNNI3sEFpv_fMbD31JfNw.png)\n",
    "\n",
    "\n",
    "Let's randomly partition the training data into training and validation set.\n",
    "\n",
    "Create `Datasets` and `Dataloader` for the training and validation data \n",
    "\n",
    "![Image](https://miro.medium.com/v2/resize:fit:720/format:webp/1*aV6AXit9w9SIOSDoKjpZ_Q.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df \n",
    "y = df.label\n",
    "\n",
    "train_df, valid_df, y_train, y_test = train_test_split(X,\n",
    "                                                       y,\n",
    "                                                       test_size=0.2,\n",
    "                                                       random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(cfg, train_df)\n",
    "valid_dataset = CustomDataset(cfg, valid_df)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset,\n",
    "                              batch_size=cfg.batch_size,\n",
    "                              shuffle=True)\n",
    "\n",
    "valid_dataloader = DataLoader(valid_dataset,\n",
    "                              batch_size=cfg.batch_size,\n",
    "                              shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the model\n",
    "\n",
    "[`timm`](https://timm.fast.ai/) - a Deep Learning library containing a collection of state-of-the-art computer vision model\n",
    "\n",
    "`timm` like `torchvision.models` but writer's comment suggest timm good for switch backbones during experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "\n",
    "cfg.n_classes = 2\n",
    "cfg.backbone = \"resnet18\"\n",
    "\n",
    "model = timm.create_model(cfg.backbone,\n",
    "                          pretrained=True,\n",
    "                          num_classes = cfg.n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timm.list_models('resnet*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(cfg.batch_size, 3, cfg.image_size, cfg.image_size)\n",
    "y = model(x)\n",
    "print(type(y))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare loss function and optimizer\n",
    "\n",
    "* a loss function (criterion)\n",
    "* an optimization algorithm (optimizer)\n",
    "* optionally a learning rate scheduler\n",
    "\n",
    "\n",
    "**Loss function** - Common loss function\n",
    "\n",
    "| Loss Function | Explanation |\n",
    "|---|---|\n",
    "| Binary Cross-Entropy (BCE) Loss | Used for binary classification tasks. This loss function compares the model's predictions with the true values. For a single sample, it is defined as `-y_true * log(y_pred) - (1 - y_true) * log(1 - y_pred)`, where `y_true` is the true binary label (0 or 1), and `y_pred` is the predicted probability from the model. The BCE loss for a set of samples is the average of the BCE losses for each individual sample. It penalizes the model heavily when it makes confident and wrong predictions. |\n",
    "| Categorical Cross-Entropy Loss | Used for multi-class classification tasks. This loss function is a generalization of the binary cross-entropy loss. It measures the dissimilarity between the predicted probability distribution and the true distribution. It is defined as `-sum(y_true * log(y_pred))` for each class, where `y_true` is the true label in one-hot encoded format, and `y_pred` is the predicted probability distribution from the model. The categorical cross-entropy loss for a set of samples is the average of the losses for each individual sample. It is suitable when each sample belongs to exactly one class. |\n",
    "| Mean Squared Error (MSE) Loss | Used for regression tasks. It measures the average squared difference between the predicted and actual values. For a set of samples, it is defined as `(1/n) * sum((y_true - y_pred)^2)`, where `y_true` is the true value, `y_pred` is the predicted value from the model, and `n` is the number of samples. MSE loss penalizes the model more heavily for larger errors due to the squaring operation. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use categorical cross-entropy loss, if you like\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optimizer** — The optimization algorithm minimizes the loss function (in our case, the cross-entropy loss). There are many different optimizers available. Let’s use a popular one: `Adam`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.learning_rate = 1e-4\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    params=model.parameters(),\n",
    "    lr = cfg.learning_rate,\n",
    "    weight_decay=0\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Learning rate scheduler**](https://towardsdatascience.com/a-visual-guide-to-learning-rate-schedulers-in-pytorch-24bbb262c863)\n",
    "\n",
    "There are many different learning rate schedulers available, but Kaggle Grandmasters recommend using cosine decay as a learning rate scheduler for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.lr_min = 1e-5\n",
    "cfg.epochs = 5\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer=optimizer,\n",
    "    T_max= np.ceil(len(train_dataloader.dataset) / cfg.batch_size) * cfg.epochs,\n",
    "    eta_min=cfg.lr_min\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`T_max` defines the half period and should be equal to the maximum number of iterations \n",
    "(`np.ceil(len(train_dataloader.dataset) /cfg.batch_size)*cfg.epochs`)\n",
    "\n",
    "Result of learning rate will look as follows over training run\n",
    "\n",
    "![Image](https://miro.medium.com/v2/resize:fit:700/1*igE55NzyCh3Z6MI587jYeg.jpeg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric\n",
    "a metric for evaluate the model's overall performance.\n",
    "\n",
    "I will you accuracy as the metric\n",
    "\n",
    ">Metric measure the model's performance after training\n",
    "\n",
    ">Loss function is used to optimize the learning function during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def calculate_metric(y, y_pred):\n",
    "    metric = accuracy_score(y, y_pred)\n",
    "    return metric"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup a training pipe-line\n",
    "\n",
    "A model is typically trained in iteration (`One iteration is called an epoch`)\n",
    "\n",
    "Training from scratch usually requires many epochs, while fine-tuning requires only few (roughly 5 to 10) epochs.\n",
    "\n",
    "1. `train_one_epoch()` for training full data \n",
    "2. `validate_one_epoch()` for validate the model on an epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(dataloader: torch.utils.data.DataLoader,\n",
    "                   model: torch.nn.Module, \n",
    "                   optimizer: torch.optim.Optimizer, \n",
    "                   scheduler: torch.optim.lr_scheduler, \n",
    "                   cfg):\n",
    "    '''\n",
    "    Train the model for an epoch.\n",
    "\n",
    "    Parameters:\n",
    "    dataloader (torch.utils.data.DataLoader): The data loader.\n",
    "    model (torch.nn.Module): The model to be trained.\n",
    "    optimizer (torch.optim.Optimizer): The optimizer.\n",
    "    scheduler (torch.optim.lr_scheduler): The learning rate scheduler.\n",
    "    cfg: The configuration with training details.\n",
    "\n",
    "    Returns:\n",
    "    metric (float): The calculated metric value.\n",
    "    loss (float): The average loss value over the epoch.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    # Set the model to training mode.\n",
    "    model.train()\n",
    "    \n",
    "    # Init lists to store y and y_pred\n",
    "    final_y = []\n",
    "    final_y_pred = []\n",
    "    final_loss = []\n",
    "    \n",
    "    # Iterate over the data\n",
    "    for step, batch in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        \n",
    "        model = model.to(cfg.device)\n",
    "        # Move the batch tensors to the same device as the model.\n",
    "        X = batch[0].float().to(cfg.device)\n",
    "        y = batch[1].to(cfg.device)\n",
    "        \n",
    "        # Reset the gradients of the model parameters\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Enable gradient calculations.\n",
    "        with torch.set_grad_enabled(True):\n",
    "            # Compute the model output.\n",
    "            y_pred = model(X)\n",
    "            \n",
    "            # Compute the loss value | Calculate the loss for the model output.\n",
    "            loss = criterion(y_pred, y)\n",
    "            \n",
    "            # Convert targets and outputs to numpy arrays and add them to the lists.\n",
    "            y = y.detach().cpu().numpy().tolist()\n",
    "            y_pred = y_pred.detach().cpu().numpy().tolist()\n",
    "            \n",
    "            # Extend the list with current batch.\n",
    "            final_y.extend(y)\n",
    "            final_y_pred.extend(y_pred)\n",
    "            final_loss.append(loss.item())\n",
    "            \n",
    "            # Perform back-propagation. | Back-propagate the error through the model.\n",
    "            loss.backward()\n",
    "            # Update the model parameters | Update the model to reduce the loss\n",
    "            optimizer.step()\n",
    "            \n",
    "        # Update the learning rate\n",
    "        scheduler.step()\n",
    "    \n",
    "    # Compute the average loss over the epoch.\n",
    "    loss = np.mean(final_loss)\n",
    "    \n",
    "    # Convert predictions to numpy arrays and \n",
    "    # get the indices of the maximum values along an axis.\n",
    "    final_y_pred = np.argmax(final_y_pred, axis=1)\n",
    "    \n",
    "    # Calculate the metric (e.g., accuracy) based on targets and predictions.\n",
    "    metric = calculate_metric(final_y, final_y_pred)\n",
    "    \n",
    "    return metric, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_one_epoch(dataloader: torch.utils.data.DataLoader, \n",
    "                      model: torch.nn.Module, \n",
    "                      cfg):\n",
    "    '''\n",
    "    Validate the model for an epoch.\n",
    "\n",
    "    Parameters:\n",
    "    dataloader (torch.utils.data.DataLoader): The data loader.\n",
    "    model (torch.nn.Module): The model to be validated.\n",
    "    cfg: The configuration with validation details.\n",
    "\n",
    "    Returns:\n",
    "    metric (float): The calculated metric value.\n",
    "    loss (float): The average loss value over the epoch.\n",
    "    '''\n",
    "    \n",
    "    # Set the model to evaluation mode. In this mode, layers like Dropout and BatchNorm \n",
    "    # will behave differently than in training mode.\n",
    "    model.eval()\n",
    "    \n",
    "    # Initialize lists to store targets (y) and predictions (y_pred).\n",
    "    final_y = []\n",
    "    final_y_pred = []\n",
    "    final_loss = []\n",
    "    \n",
    "    # Iterate over data\n",
    "    for step, batch in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        \n",
    "        model = model.to(cfg.device)\n",
    "        # Move the batch tensors to the same device as the model.\n",
    "        X = batch[0].to(cfg.device).float()\n",
    "        y = batch[1].to(cfg.device).long()\n",
    "        \n",
    "        # with torch.inference_mode():\n",
    "        with torch.no_grad():\n",
    "            # Compute the model output | forward pass of the input through the model\n",
    "            y_pred = model(X)\n",
    "            # Compute the loss value.\n",
    "            loss = criterion(y_pred, y)\n",
    "            # Convert targets and outputs to numpy arrays and add them to the lists.\n",
    "            y = y.detach().cpu().numpy().tolist()\n",
    "            y_pred = y_pred.detach().cpu().numpy().tolist()\n",
    "            \n",
    "            # Extend original list\n",
    "            final_y.extend(y)\n",
    "            final_y_pred.extend(y_pred)\n",
    "            final_loss.append(loss.item())\n",
    "            \n",
    "    # Compute the average loss over the epoch.\n",
    "    loss = np.mean(final_loss)\n",
    "\n",
    "    # Convert predictions to numpy arrays and get the indices of the maximum values along an axis.\n",
    "    final_y_pred = np.argmax(final_y_pred, axis=1)\n",
    "\n",
    "    # Calculate the metric (e.g., accuracy) based on targets and predictions.\n",
    "    metric = calculate_metric(final_y, final_y_pred)\n",
    "\n",
    "    return metric, loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Training Mode**: This is set by calling `model.train()`. In this mode, layers like Dropout and BatchNorm have an effect on the model's output. For Dropout, this means that during training, certain neurons will be randomly \"dropped out\", or turned off, which helps prevent overfitting. For BatchNorm, this means that the layer will normalize its inputs using both the current batch's mean and variance, as well as keeping track of running estimates of these statistics for use in testing.\n",
    "\n",
    "* **Evaluation Mode**: This is set by calling `model.eval()`. In this mode, the Dropout layers will not \"drop out\" any neurons, and the BatchNorm layers will use the running estimates of mean and variance collected during training rather than the batch's mean and variance. This is because we want to use the full learned capabilities of the model for evaluation and testing, and we don't want the model's output to vary from one run to another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding data augmentation\n",
    "\n",
    "If the metric of training and validation are significantly different, this indicates that the model is over-fitting to the training data. Over-fitting occurs when a model is trained on only a few examples and learns irrelevant details or noise from the training data.\n",
    "\n",
    "You can use data augmentation to overcome over-fitting by generate additional taining data by randomly transforming existing images make model generalize better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_soft = A.Compose([A.Resize(cfg.image_size, cfg.image_size),\n",
    "                           A.Rotate(p=0.6, limit=[-10,10]),\n",
    "                           A.HorizontalFlip(p=0.6),\n",
    "                           ToTensorV2()])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`p` parameter is control probability of applying the augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "cfg.seed = 42\n",
    "\n",
    "def fit(model: torch.nn.Module, \n",
    "        optimizer: torch.optim.Optimizer, \n",
    "        cfg,\n",
    "        scheduler: torch.optim.lr_scheduler,\n",
    "        train_dataloader: torch.utils.data.DataLoader, \n",
    "        valid_dataloader: Optional[torch.utils.data.DataLoader] =None):\n",
    "    \n",
    "    '''\n",
    "    Fits the model for a given number of epochs.\n",
    "\n",
    "    Parameters:\n",
    "    model (torch.nn.Module): The model to be trained.\n",
    "    optimizer (torch.optim.Optimizer): The optimizer.\n",
    "    scheduler (torch.optim.lr_scheduler): The learning rate scheduler.\n",
    "    cfg: The configuration with training details.\n",
    "    train_dataloader (torch.utils.data.DataLoader): The data loader for training.\n",
    "    valid_dataloader (Optional[torch.utils.data.DataLoader]): The data loader for validation. If None, no validation is performed.\n",
    "\n",
    "    Returns:\n",
    "    acc_list (List[float]): The list of accuracy values for each training epoch.\n",
    "    loss_list (List[float]): The list of loss values for each training epoch.\n",
    "    val_acc_list (List[float]): The list of accuracy values for each validation epoch.\n",
    "    val_loss_list (List[float]): The list of loss values for each validation epoch.\n",
    "    model (torch.nn.Module): The trained model.\n",
    "    '''\n",
    "    \n",
    "    # Initialize lists to store accuracy and loss values for training and validation.\n",
    "    acc_list = []\n",
    "    loss_list = []\n",
    "    val_acc_list = []\n",
    "    val_loss_list = []\n",
    "    \n",
    "    # Train for a specified number of epochs.\n",
    "    for epoch in range(cfg.epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{cfg.epochs}\")\n",
    "        \n",
    "        # Set a new seed for each epoch to ensure different randomization.\n",
    "        set_seed(cfg.seed + epoch)\n",
    "        \n",
    "        # Train the model for one epoch and get the accuracy and loss.\n",
    "        acc, loss = train_one_epoch(train_dataloader,\n",
    "                                   model, \n",
    "                                   optimizer=optimizer,\n",
    "                                   scheduler=scheduler,\n",
    "                                   cfg=cfg)\n",
    "        \n",
    "        # If a validation dataloader is provided, validate the model and get the accuracy and loss.\n",
    "        if valid_dataloader:\n",
    "            val_acc, val_loss = validate_one_epoch(valid_dataloader, model, cfg)\n",
    "        \n",
    "        # Print the training loss and accuracy for this epoch. \n",
    "        print(f\"Loss: {loss:.4f} Acc: {acc:.4f}\")\n",
    "        \n",
    "        # Add the accuracy and loss for this epoch to the lists.\n",
    "        acc_list.append(acc)\n",
    "        loss_list.append(loss)\n",
    "        \n",
    "        # If a validation dataloader was provided, print the validation loss and accuracy and add them to the lists.\n",
    "        if valid_dataloader:\n",
    "            print(f\"Val Loss: {val_loss:.4f} Val Acc: {val_acc:.4f}\")\n",
    "            val_acc_list.append(val_acc)\n",
    "            val_loss_list.append(val_loss)\n",
    "            \n",
    "            \n",
    "    # Return the lists of accuracy and loss for both training and validation, as well as the trained model.\n",
    "    return acc_list, loss_list, val_acc_list, val_loss_list, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = fit(model=model,\n",
    "    optimizer=optimizer,\n",
    "    cfg=cfg,\n",
    "    scheduler=scheduler,\n",
    "    train_dataloader=train_dataloader,\n",
    "    valid_dataloader=valid_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
